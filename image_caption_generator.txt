!pip install transformers
!pip install streamlit


from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
import torch
from PIL import Image

#loading the model
model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
feature_extractor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

#sets CUDA if GPU is being used else sets device as CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

#initialize parameters
max_length = 16                  #maximum length of final description cannot exceed 16 tokens
num_beams = 5                    #number of beams during beam search = 5
num_return_sequences = 4         #Change this to desired number of captions to be generated per image
#NOTE: ensure that num_return_sequences is ALWAYS lesser than the num_beams variable

#gen_kwargs is a dictionary that contains parametrs to control the decoding process.
gen_kwargs = {"max_length": max_length, "num_beams": num_beams, "num_return_sequences":num_return_sequences}

def predict_step(image_paths):
  images = []
  for image_path in image_paths:
    i_image = Image.open('/content/tree.jpg')     #Image.open uses PIL library and the path of the image must be inserted here
    if i_image.mode != "RGB":                     #if image is not RGB, ie. B&W, it converts it to RGB.
      i_image = i_image.convert(mode="RGB")

    images.append(i_image)

  pixel_values = feature_extractor(images=images, return_tensors="pt").pixel_values     #feature extraction
  pixel_values = pixel_values.to(device)        #move the pixel values to the respective device mentioned earlier

  output_ids = model.generate(pixel_values, **gen_kwargs)

  captions=[]
  for output_sequence in output_ids:   #iterate through all output ids by running a for loop
    preds = tokenizer.decode(output_sequence, skip_special_tokens=True)
    #tokenizer.decode() method is used to convert the output sequence of tokens generated by the language model into a human-readable sentence or text.
    preds = preds.strip()       #strip of any unnecessary white spaces
    captions.append(preds)
  return captions

image_paths=['/content/tree.jpg']
preds=predict_step(image_paths)
print(preds)

